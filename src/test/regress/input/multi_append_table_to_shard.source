--
-- MULTI_APPEND_TABLE_TO_SHARD
--
-- Initialize tables to join
CREATE TABLE multi_append_table_to_shard_right
(
	right_number INTEGER not null,
	right_text TEXT not null
);
SELECT master_create_distributed_table('multi_append_table_to_shard_right', 'right_number', 'append');

CREATE TABLE multi_append_table_to_shard_left
(
	left_number INTEGER not null,
	left_text TEXT not null
);
SELECT master_create_distributed_table('multi_append_table_to_shard_left', 'left_number', 'append');

CREATE TABLE multi_append_table_to_shard_right_hash
(
	right_number INTEGER not null,
	right_text TEXT not null
);
SELECT master_create_distributed_table('multi_append_table_to_shard_right_hash', 'right_number', 'hash');
SELECT master_create_worker_shards('multi_append_table_to_shard_right_hash', 1, 1);

-- Replicate 'left' table on both workers
SELECT set_config('citus.shard_replication_factor', '2', false);
\STAGE multi_append_table_to_shard_left FROM '@abs_srcdir@/data/agg.data'
\STAGE multi_append_table_to_shard_left FROM '@abs_srcdir@/data/agg.data'

-- Place 'right' table only on the primary worker
SELECT set_config('citus.shard_replication_factor', '1', false);
\STAGE multi_append_table_to_shard_right FROM '@abs_srcdir@/data/agg.data'

-- Reset shard replication factor to ensure tasks will be assigned to both workers
SELECT set_config('citus.shard_replication_factor', '2', false);

-- All 8 rows in left table match a row in right table
SELECT	COUNT(*)
FROM	multi_append_table_to_shard_left,
		multi_append_table_to_shard_right
WHERE	left_number = right_number;

-- Now append more data to the 'right' table
CREATE TABLE multi_append_table_to_shard_stage
(
	number INTEGER not null,
	text TEXT not null
);

COPY multi_append_table_to_shard_stage FROM '@abs_srcdir@/data/agg.data';

SELECT	master_append_table_to_shard(shardid, 'multi_append_table_to_shard_stage', 'localhost', 57636)
FROM
		pg_dist_shard
WHERE	'multi_append_table_to_shard_right'::regclass::oid = logicalrelid;

-- Only the primary worker will see the new matches, as the secondary still uses a cached shard
SELECT	COUNT(*)
FROM	multi_append_table_to_shard_left,
		multi_append_table_to_shard_right
WHERE	left_number = right_number;

-- Now add a lot of data to ensure we increase the size on disk
DELETE FROM multi_append_table_to_shard_stage;
COPY multi_append_table_to_shard_stage FROM '@abs_srcdir@/data/large_records.data' with delimiter '|';

SELECT	master_append_table_to_shard(shardid, 'multi_append_table_to_shard_stage', 'localhost', 57636)
FROM
		pg_dist_shard
WHERE	'multi_append_table_to_shard_right'::regclass::oid = logicalrelid;

-- This join will refresh the shard on the secondary, all 8 rows in the left table will match twice (16)
SELECT	COUNT(*)
FROM	multi_append_table_to_shard_left,
		multi_append_table_to_shard_right
WHERE	left_number = right_number;

-- Check that we error out if we try to append data to a hash partitioned table.
SELECT master_create_empty_shard('multi_append_table_to_shard_right_hash');

SELECT	master_append_table_to_shard(shardid, 'multi_append_table_to_shard_stage', 'localhost', 57636)
FROM
		pg_dist_shard
WHERE	'multi_append_table_to_shard_right_hash'::regclass::oid = logicalrelid;

-- Clean up after test
SELECT master_apply_delete_command('DELETE FROM multi_append_table_to_shard_right');
SELECT master_apply_delete_command('DELETE FROM multi_append_table_to_shard_left');
DROP TABLE multi_append_table_to_shard_stage;
DROP TABLE multi_append_table_to_shard_right;
DROP TABLE multi_append_table_to_shard_left;

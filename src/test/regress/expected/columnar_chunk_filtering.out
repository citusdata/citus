--
-- Test chunk filtering in columnar using min/max values in stripe skip lists.
--
--
-- filtered_row_count returns number of rows filtered by the WHERE clause.
-- If chunks get filtered by columnar, less rows are passed to WHERE
-- clause, so this function should return a lower number.
--
CREATE OR REPLACE FUNCTION filtered_row_count (query text) RETURNS bigint AS
$$
    DECLARE
        result bigint;
        rec text;
    BEGIN
        result := 0;

        FOR rec IN EXECUTE 'EXPLAIN ANALYZE ' || query LOOP
            IF rec ~ '^\s+Rows Removed by Filter' then
                result := regexp_replace(rec, '[^0-9]*', '', 'g');
            END IF;
        END LOOP;

        RETURN result;
    END;
$$ LANGUAGE PLPGSQL;
-- Create and load data
-- chunk_group_row_limit '1000', stripe_row_limit '2000'
set columnar.stripe_row_limit = 2000;
set columnar.chunk_group_row_limit = 1000;
CREATE TABLE test_chunk_filtering (a int)
    USING columnar;
INSERT INTO test_chunk_filtering SELECT generate_series(1,10000);
-- Verify that filtered_row_count is less than 1000 for the following queries
SELECT filtered_row_count('SELECT count(*) FROM test_chunk_filtering');
 filtered_row_count
---------------------------------------------------------------------
                  0
(1 row)

SELECT filtered_row_count('SELECT count(*) FROM test_chunk_filtering WHERE a < 200');
 filtered_row_count
---------------------------------------------------------------------
                801
(1 row)

SELECT filtered_row_count('SELECT count(*) FROM test_chunk_filtering WHERE a > 200');
 filtered_row_count
---------------------------------------------------------------------
                200
(1 row)

SELECT filtered_row_count('SELECT count(*) FROM test_chunk_filtering WHERE a < 9900');
 filtered_row_count
---------------------------------------------------------------------
                101
(1 row)

SELECT filtered_row_count('SELECT count(*) FROM test_chunk_filtering WHERE a > 9900');
 filtered_row_count
---------------------------------------------------------------------
                900
(1 row)

SELECT filtered_row_count('SELECT count(*) FROM test_chunk_filtering WHERE a < 0');
 filtered_row_count
---------------------------------------------------------------------
                  0
(1 row)

-- Verify that filtered_row_count is less than 2000 for the following queries
SELECT filtered_row_count('SELECT count(*) FROM test_chunk_filtering WHERE a BETWEEN 1 AND 10');
 filtered_row_count
---------------------------------------------------------------------
                990
(1 row)

SELECT filtered_row_count('SELECT count(*) FROM test_chunk_filtering WHERE a BETWEEN 990 AND 2010');
 filtered_row_count
---------------------------------------------------------------------
               1979
(1 row)

SELECT filtered_row_count('SELECT count(*) FROM test_chunk_filtering WHERE a BETWEEN -10 AND 0');
 filtered_row_count
---------------------------------------------------------------------
                  0
(1 row)

-- Load data for second time and verify that filtered_row_count is exactly twice as before
INSERT INTO test_chunk_filtering SELECT generate_series(1,10000);
SELECT filtered_row_count('SELECT count(*) FROM test_chunk_filtering WHERE a < 200');
 filtered_row_count
---------------------------------------------------------------------
               1602
(1 row)

SELECT filtered_row_count('SELECT count(*) FROM test_chunk_filtering WHERE a < 0');
 filtered_row_count
---------------------------------------------------------------------
                  0
(1 row)

SELECT filtered_row_count('SELECT count(*) FROM test_chunk_filtering WHERE a BETWEEN 990 AND 2010');
 filtered_row_count
---------------------------------------------------------------------
               3958
(1 row)

set columnar.stripe_row_limit to default;
set columnar.chunk_group_row_limit to default;
-- Verify that we are fine with collations which use a different alphabet order
CREATE TABLE collation_chunk_filtering_test(A text collate "da_DK")
    USING columnar;
COPY collation_chunk_filtering_test FROM STDIN;
SELECT * FROM collation_chunk_filtering_test WHERE A > 'B';
 a
---------------------------------------------------------------------
 Ã…
(1 row)

CREATE TABLE simple_chunk_filtering(i int) USING COLUMNAR;
INSERT INTO simple_chunk_filtering SELECT generate_series(0,234567);
EXPLAIN (analyze on, costs off, timing off, summary off)
  SELECT * FROM simple_chunk_filtering WHERE i > 123456;
                                    QUERY PLAN
---------------------------------------------------------------------
 Custom Scan (ColumnarScan) on simple_chunk_filtering (actual rows=111111 loops=1)
   Filter: (i > 123456)
   Rows Removed by Filter: 3457
   Columnar Chunk Groups Removed by Filter: 12
   Columnar Projected Columns: i
(5 rows)

SET columnar.enable_qual_pushdown = false;
EXPLAIN (analyze on, costs off, timing off, summary off)
  SELECT * FROM simple_chunk_filtering WHERE i > 123456;
                                    QUERY PLAN
---------------------------------------------------------------------
 Custom Scan (ColumnarScan) on simple_chunk_filtering (actual rows=111111 loops=1)
   Filter: (i > 123456)
   Rows Removed by Filter: 123457
   Columnar Chunk Groups Removed by Filter: 0
   Columnar Projected Columns: i
(5 rows)

SET columnar.enable_qual_pushdown TO DEFAULT;
-- https://github.com/citusdata/citus/issues/4555
TRUNCATE simple_chunk_filtering;
INSERT INTO simple_chunk_filtering SELECT generate_series(0,200000);
COPY (SELECT * FROM simple_chunk_filtering WHERE i > 180000) TO '/dev/null';
EXPLAIN (analyze on, costs off, timing off, summary off)
  SELECT * FROM simple_chunk_filtering WHERE i > 180000;
                                    QUERY PLAN
---------------------------------------------------------------------
 Custom Scan (ColumnarScan) on simple_chunk_filtering (actual rows=20000 loops=1)
   Filter: (i > 180000)
   Rows Removed by Filter: 1
   Columnar Chunk Groups Removed by Filter: 18
   Columnar Projected Columns: i
(5 rows)

DROP TABLE simple_chunk_filtering;
CREATE TABLE multi_column_chunk_filtering(a int, b int) USING columnar;
INSERT INTO multi_column_chunk_filtering SELECT i,i+1 FROM generate_series(0,234567) i;
EXPLAIN (analyze on, costs off, timing off, summary off)
  SELECT count(*) FROM multi_column_chunk_filtering WHERE a > 50000;
                                          QUERY PLAN
---------------------------------------------------------------------
 Aggregate (actual rows=1 loops=1)
   ->  Custom Scan (ColumnarScan) on multi_column_chunk_filtering (actual rows=184567 loops=1)
         Filter: (a > 50000)
         Rows Removed by Filter: 1
         Columnar Chunk Groups Removed by Filter: 5
         Columnar Projected Columns: a
(6 rows)

EXPLAIN (analyze on, costs off, timing off, summary off)
  SELECT count(*) FROM multi_column_chunk_filtering WHERE a > 50000 AND b > 50000;
                                          QUERY PLAN
---------------------------------------------------------------------
 Aggregate (actual rows=1 loops=1)
   ->  Custom Scan (ColumnarScan) on multi_column_chunk_filtering (actual rows=184567 loops=1)
         Filter: ((a > 50000) AND (b > 50000))
         Rows Removed by Filter: 1
         Columnar Chunk Groups Removed by Filter: 5
         Columnar Projected Columns: a, b
(6 rows)

-- make next tests faster
TRUNCATE multi_column_chunk_filtering;
INSERT INTO multi_column_chunk_filtering SELECT generate_series(0,5);
EXPLAIN (analyze on, costs off, timing off, summary off)
  SELECT b FROM multi_column_chunk_filtering WHERE a > 50000 AND b > 50000;
                                     QUERY PLAN
---------------------------------------------------------------------
 Custom Scan (ColumnarScan) on multi_column_chunk_filtering (actual rows=0 loops=1)
   Filter: ((a > 50000) AND (b > 50000))
   Columnar Chunk Groups Removed by Filter: 1
   Columnar Projected Columns: a, b
(4 rows)

EXPLAIN (analyze on, costs off, timing off, summary off)
  SELECT b, a FROM multi_column_chunk_filtering WHERE b > 50000;
                                     QUERY PLAN
---------------------------------------------------------------------
 Custom Scan (ColumnarScan) on multi_column_chunk_filtering (actual rows=0 loops=1)
   Filter: (b > 50000)
   Rows Removed by Filter: 6
   Columnar Chunk Groups Removed by Filter: 0
   Columnar Projected Columns: a, b
(5 rows)

EXPLAIN (analyze on, costs off, timing off, summary off)
  SELECT FROM multi_column_chunk_filtering WHERE a > 50000;
                                     QUERY PLAN
---------------------------------------------------------------------
 Custom Scan (ColumnarScan) on multi_column_chunk_filtering (actual rows=0 loops=1)
   Filter: (a > 50000)
   Columnar Chunk Groups Removed by Filter: 1
   Columnar Projected Columns: a
(4 rows)

EXPLAIN (analyze on, costs off, timing off, summary off)
  SELECT FROM multi_column_chunk_filtering;
                                     QUERY PLAN
---------------------------------------------------------------------
 Custom Scan (ColumnarScan) on multi_column_chunk_filtering (actual rows=6 loops=1)
   Columnar Chunk Groups Removed by Filter: 0
   Columnar Projected Columns: <columnar optimized out all columns>
(3 rows)

BEGIN;
  ALTER TABLE multi_column_chunk_filtering DROP COLUMN a;
  ALTER TABLE multi_column_chunk_filtering DROP COLUMN b;
  EXPLAIN (analyze on, costs off, timing off, summary off)
  SELECT * FROM multi_column_chunk_filtering;
                                     QUERY PLAN
---------------------------------------------------------------------
 Custom Scan (ColumnarScan) on multi_column_chunk_filtering (actual rows=6 loops=1)
   Columnar Chunk Groups Removed by Filter: 0
   Columnar Projected Columns: <columnar optimized out all columns>
(3 rows)

ROLLBACK;
CREATE TABLE another_columnar_table(x int, y int) USING columnar;
INSERT INTO another_columnar_table SELECT generate_series(0,5);
EXPLAIN (analyze on, costs off, timing off, summary off)
  SELECT a, y FROM multi_column_chunk_filtering, another_columnar_table WHERE x > 1;
                                        QUERY PLAN
---------------------------------------------------------------------
 Nested Loop (actual rows=24 loops=1)
   ->  Custom Scan (ColumnarScan) on another_columnar_table (actual rows=4 loops=1)
         Filter: (x > 1)
         Rows Removed by Filter: 2
         Columnar Chunk Groups Removed by Filter: 0
         Columnar Projected Columns: x, y
   ->  Custom Scan (ColumnarScan) on multi_column_chunk_filtering (actual rows=6 loops=4)
         Columnar Chunk Groups Removed by Filter: 0
         Columnar Projected Columns: a
(9 rows)

EXPLAIN (costs off, timing off, summary off)
  SELECT y, * FROM another_columnar_table;
                      QUERY PLAN
---------------------------------------------------------------------
 Custom Scan (ColumnarScan) on another_columnar_table
   Columnar Projected Columns: x, y
(2 rows)

EXPLAIN (costs off, timing off, summary off)
  SELECT *, x FROM another_columnar_table;
                      QUERY PLAN
---------------------------------------------------------------------
 Custom Scan (ColumnarScan) on another_columnar_table
   Columnar Projected Columns: x, y
(2 rows)

EXPLAIN (costs off, timing off, summary off)
  SELECT y, another_columnar_table FROM another_columnar_table;
                      QUERY PLAN
---------------------------------------------------------------------
 Custom Scan (ColumnarScan) on another_columnar_table
   Columnar Projected Columns: x, y
(2 rows)

EXPLAIN (costs off, timing off, summary off)
  SELECT another_columnar_table, x FROM another_columnar_table;
                      QUERY PLAN
---------------------------------------------------------------------
 Custom Scan (ColumnarScan) on another_columnar_table
   Columnar Projected Columns: x, y
(2 rows)

DROP TABLE multi_column_chunk_filtering, another_columnar_table;
--
-- https://github.com/citusdata/citus/issues/4780
--
create table part_table (id int) partition by range (id);
create table part_1_row partition of part_table for values from (150000) to (160000);
create table part_2_columnar partition of part_table for values from (0) to (150000) using columnar;
insert into part_table select generate_series(1,159999);
select filtered_row_count('select count(*) from part_table where id > 75000');
 filtered_row_count
---------------------------------------------------------------------
               5000
(1 row)

drop table part_table;

CREATE SCHEMA null_dist_key_udfs;
SET search_path TO null_dist_key_udfs;
SET citus.next_shard_id TO 1820000;
SET citus.shard_count TO 32;
SET citus.shard_replication_factor TO 1;
ALTER SEQUENCE pg_catalog.pg_dist_colocationid_seq RESTART 198000;
SET client_min_messages TO ERROR;
SELECT 1 FROM citus_add_node('localhost', :master_port, groupid=>0);
 ?column?
---------------------------------------------------------------------
        1
(1 row)

RESET client_min_messages;
-- test some other udf's with single shard tables
CREATE TABLE null_dist_key_table(a int);
SELECT create_distributed_table('null_dist_key_table', null, colocate_with=>'none', distribution_type=>null);
 create_distributed_table
---------------------------------------------------------------------

(1 row)

SELECT truncate_local_data_after_distributing_table('null_dist_key_table');
 truncate_local_data_after_distributing_table
---------------------------------------------------------------------

(1 row)

-- should work --
-- insert some data & create an index for table size udf's
INSERT INTO null_dist_key_table VALUES (1), (2), (3);
CREATE INDEX null_dist_key_idx ON null_dist_key_table(a);
SELECT citus_table_size('null_dist_key_table');
 citus_table_size
---------------------------------------------------------------------
             8192
(1 row)

SELECT citus_total_relation_size('null_dist_key_table');
 citus_total_relation_size
---------------------------------------------------------------------
                     24576
(1 row)

SELECT citus_relation_size('null_dist_key_table');
 citus_relation_size
---------------------------------------------------------------------
                8192
(1 row)

SELECT * FROM pg_catalog.citus_shard_sizes() WHERE table_name LIKE '%null_dist_key_table%';
                   table_name                   | size
---------------------------------------------------------------------
 null_dist_key_udfs.null_dist_key_table_1820000 | 24576
(1 row)

BEGIN;
  SELECT lock_relation_if_exists('null_dist_key_table', 'ACCESS SHARE');
 lock_relation_if_exists
---------------------------------------------------------------------
 t
(1 row)

  SELECT count(*) FROM pg_locks where relation='null_dist_key_table'::regclass;
 count
---------------------------------------------------------------------
     1
(1 row)

COMMIT;
SELECT partmethod, repmodel FROM pg_dist_partition WHERE logicalrelid = 'null_dist_key_table'::regclass;
 partmethod | repmodel
---------------------------------------------------------------------
 n          | s
(1 row)

SELECT master_get_table_ddl_events('null_dist_key_table');
                               master_get_table_ddl_events
---------------------------------------------------------------------
 CREATE TABLE null_dist_key_udfs.null_dist_key_table (a integer) USING heap
 ALTER TABLE null_dist_key_udfs.null_dist_key_table OWNER TO postgres
 CREATE INDEX null_dist_key_idx ON null_dist_key_udfs.null_dist_key_table USING btree (a)
(3 rows)

SELECT column_to_column_name(logicalrelid, partkey)
FROM pg_dist_partition WHERE logicalrelid = 'null_dist_key_table'::regclass;
 column_to_column_name
---------------------------------------------------------------------

(1 row)

SELECT column_name_to_column('null_dist_key_table', 'a');
                                                  column_name_to_column
---------------------------------------------------------------------
 {VAR :varno 1 :varattno 1 :vartype 23 :vartypmod -1 :varcollid 0 :varlevelsup 0 :varnosyn 1 :varattnosyn 1 :location -1}
(1 row)

SELECT master_update_shard_statistics(shardid)
FROM (SELECT shardid FROM pg_dist_shard WHERE logicalrelid='null_dist_key_table'::regclass) as shardid;
 master_update_shard_statistics
---------------------------------------------------------------------
                           8192
(1 row)

SELECT truncate_local_data_after_distributing_table('null_dist_key_table');
 truncate_local_data_after_distributing_table
---------------------------------------------------------------------

(1 row)

-- should return a single element array that only includes its own shard id
SELECT shardid=unnest(get_colocated_shard_array(shardid))
FROM (SELECT shardid FROM pg_dist_shard WHERE logicalrelid='null_dist_key_table'::regclass) as shardid;
 ?column?
---------------------------------------------------------------------
 t
(1 row)

BEGIN;
  SELECT master_remove_partition_metadata('null_dist_key_table'::regclass::oid, 'null_dist_key_udfs', 'null_dist_key_table');
 master_remove_partition_metadata
---------------------------------------------------------------------

(1 row)

  -- should print 0
  select count(*) from pg_dist_partition where logicalrelid='null_dist_key_table'::regclass;
 count
---------------------------------------------------------------------
     0
(1 row)

ROLLBACK;
SELECT master_create_empty_shard('null_dist_key_table');
ERROR:  relation "null_dist_key_table" is a single shard table
DETAIL:  We currently don't support creating shards on single shard tables
-- return true
SELECT citus_table_is_visible('null_dist_key_table'::regclass::oid);
 citus_table_is_visible
---------------------------------------------------------------------
 t
(1 row)

-- return false
SELECT relation_is_a_known_shard('null_dist_key_table');
 relation_is_a_known_shard
---------------------------------------------------------------------
 f
(1 row)

-- return | false | true |
SELECT citus_table_is_visible(tableName::regclass::oid), relation_is_a_known_shard(tableName::regclass)
FROM (SELECT tableName FROM pg_catalog.pg_tables WHERE tablename LIKE 'null_dist_key_table%') as tableName;
 citus_table_is_visible | relation_is_a_known_shard
---------------------------------------------------------------------
 t                      | f
(1 row)

-- should fail, maybe support in the future
SELECT create_reference_table('null_dist_key_table');
ERROR:  table "null_dist_key_table" is already distributed
SELECT create_distributed_table('null_dist_key_table', 'a');
ERROR:  table "null_dist_key_table" is already distributed
SELECT create_distributed_table_concurrently('null_dist_key_table', 'a');
ERROR:  table "null_dist_key_table" is already distributed
SELECT citus_add_local_table_to_metadata('null_dist_key_table');
ERROR:  table "null_dist_key_table" is already distributed
-- test altering distribution column, fails for single shard tables
SELECT alter_distributed_table('null_dist_key_table', distribution_column := 'a');
ERROR:  relation null_dist_key_table should be a hash distributed table
-- test altering shard count, fails for single shard tables
SELECT alter_distributed_table('null_dist_key_table', shard_count := 6);
ERROR:  relation null_dist_key_table should be a hash distributed table
-- test shard splitting udf, fails for single shard tables
SELECT nodeid AS worker_1_node FROM pg_dist_node WHERE nodeport=:worker_1_port \gset
SELECT nodeid AS worker_2_node FROM pg_dist_node WHERE nodeport=:worker_2_port \gset
SELECT citus_split_shard_by_split_points(
	1820000,
	ARRAY['-1073741826'],
	ARRAY[:worker_1_node, :worker_2_node],
    'block_writes');
ERROR:  Cannot split shard as operation is only supported for hash distributed tables.
SELECT colocationid FROM pg_dist_partition WHERE logicalrelid::text LIKE '%null_dist_key_table%';
 colocationid
---------------------------------------------------------------------
       198000
(1 row)

-- test alter_table_set_access_method and verify it doesn't change the colocation id
SELECT alter_table_set_access_method('null_dist_key_table', 'columnar');
NOTICE:  creating a new table for null_dist_key_udfs.null_dist_key_table
NOTICE:  moving the data of null_dist_key_udfs.null_dist_key_table
NOTICE:  dropping the old null_dist_key_udfs.null_dist_key_table
NOTICE:  renaming the new table to null_dist_key_udfs.null_dist_key_table
 alter_table_set_access_method
---------------------------------------------------------------------

(1 row)

SELECT colocationid FROM pg_dist_partition WHERE logicalrelid::text LIKE '%null_dist_key_table%';
 colocationid
---------------------------------------------------------------------
       198000
(1 row)

-- undistribute
SELECT undistribute_table('null_dist_key_table');
NOTICE:  creating a new table for null_dist_key_udfs.null_dist_key_table
NOTICE:  moving the data of null_dist_key_udfs.null_dist_key_table
NOTICE:  dropping the old null_dist_key_udfs.null_dist_key_table
NOTICE:  renaming the new table to null_dist_key_udfs.null_dist_key_table
 undistribute_table
---------------------------------------------------------------------

(1 row)

-- verify that the metadata is gone
SELECT COUNT(*) = 0 FROM pg_dist_partition WHERE logicalrelid::text LIKE '%null_dist_key_table%';
 ?column?
---------------------------------------------------------------------
 t
(1 row)

SELECT COUNT(*) = 0 FROM pg_dist_placement WHERE shardid IN (SELECT shardid FROM pg_dist_shard WHERE logicalrelid::text LIKE '%null_dist_key_table%');
 ?column?
---------------------------------------------------------------------
 t
(1 row)

SELECT COUNT(*) = 0 FROM pg_dist_shard WHERE logicalrelid::text LIKE '%null_dist_key_table%';
 ?column?
---------------------------------------------------------------------
 t
(1 row)

-- test update_distributed_table_colocation
CREATE TABLE update_col_1 (a INT);
CREATE TABLE update_col_2 (a INT);
CREATE TABLE update_col_3 (a INT);
-- create colocated single shard distributed tables, so the shards will be
-- in the same worker node
SELECT create_distributed_table ('update_col_1', null, colocate_with:='none');
 create_distributed_table
---------------------------------------------------------------------

(1 row)

SELECT create_distributed_table ('update_col_2', null, colocate_with:='update_col_1');
 create_distributed_table
---------------------------------------------------------------------

(1 row)

-- now create a third single shard distributed table that is not colocated,
-- with the new colocation id the new table will be in the other worker node
SELECT create_distributed_table ('update_col_3', null, colocate_with:='none');
 create_distributed_table
---------------------------------------------------------------------

(1 row)

-- make sure nodes are correct
SELECT c1.nodeport = c2.nodeport AS same_node
FROM citus_shards c1, citus_shards c2, pg_dist_node p1, pg_dist_node p2
WHERE c1.table_name::text = 'update_col_1' AND c2.table_name::text = 'update_col_2' AND
      p1.nodeport = c1.nodeport AND p2.nodeport = c2.nodeport AND
      p1.noderole = 'primary' AND p2.noderole = 'primary';
 same_node
---------------------------------------------------------------------
 t
(1 row)

SELECT c1.nodeport = c2.nodeport AS same_node
FROM citus_shards c1, citus_shards c2, pg_dist_node p1, pg_dist_node p2
WHERE c1.table_name::text = 'update_col_1' AND c2.table_name::text = 'update_col_3' AND
      p1.nodeport = c1.nodeport AND p2.nodeport = c2.nodeport AND
      p1.noderole = 'primary' AND p2.noderole = 'primary';
 same_node
---------------------------------------------------------------------
 f
(1 row)

-- and the update_col_1 and update_col_2 are colocated
SELECT c1.colocation_id = c2.colocation_id AS colocated
FROM public.citus_tables c1, public.citus_tables c2
WHERE c1.table_name::text = 'update_col_1' AND c2.table_name::text = 'update_col_2';
 colocated
---------------------------------------------------------------------
 t
(1 row)

-- break the colocation
SELECT update_distributed_table_colocation('update_col_2', colocate_with:='none');
 update_distributed_table_colocation
---------------------------------------------------------------------

(1 row)

SELECT c1.colocation_id = c2.colocation_id AS colocated
FROM public.citus_tables c1, public.citus_tables c2
WHERE c1.table_name::text = 'update_col_1' AND c2.table_name::text = 'update_col_2';
 colocated
---------------------------------------------------------------------
 f
(1 row)

-- re-colocate, the shards were already in the same node
SELECT update_distributed_table_colocation('update_col_2', colocate_with:='update_col_1');
 update_distributed_table_colocation
---------------------------------------------------------------------

(1 row)

SELECT c1.colocation_id = c2.colocation_id AS colocated
FROM public.citus_tables c1, public.citus_tables c2
WHERE c1.table_name::text = 'update_col_1' AND c2.table_name::text = 'update_col_2';
 colocated
---------------------------------------------------------------------
 t
(1 row)

-- update_col_1 and update_col_3 are not colocated, because they are not in the some node
SELECT c1.colocation_id = c2.colocation_id AS colocated
FROM public.citus_tables c1, public.citus_tables c2
WHERE c1.table_name::text = 'update_col_1' AND c2.table_name::text = 'update_col_3';
 colocated
---------------------------------------------------------------------
 f
(1 row)

-- they should not be able to be colocated since the shards are in different nodes
SELECT update_distributed_table_colocation('update_col_3', colocate_with:='update_col_1');
ERROR:  cannot colocate tables update_col_1 and update_col_3
DETAIL:  Shard xxxxx of update_col_1 and shard xxxxx of update_col_3 are not colocated.
SELECT c1.colocation_id = c2.colocation_id AS colocated
FROM public.citus_tables c1, public.citus_tables c2
WHERE c1.table_name::text = 'update_col_1' AND c2.table_name::text = 'update_col_3';
 colocated
---------------------------------------------------------------------
 f
(1 row)

-- hash distributed and single shard distributed tables cannot be colocated
CREATE TABLE update_col_4 (a INT);
SELECT create_distributed_table ('update_col_4', 'a', colocate_with:='none');
 create_distributed_table
---------------------------------------------------------------------

(1 row)

SELECT update_distributed_table_colocation('update_col_1', colocate_with:='update_col_4');
ERROR:  cannot colocate tables update_col_4 and update_col_1
DETAIL:  Distribution column types don't match for update_col_4 and update_col_1.
SELECT update_distributed_table_colocation('update_col_4', colocate_with:='update_col_1');
ERROR:  cannot colocate tables update_col_1 and update_col_4
DETAIL:  Distribution column types don't match for update_col_1 and update_col_4.
-- test columnar UDFs
CREATE TABLE columnar_tbl (a INT) USING COLUMNAR;
SELECT create_distributed_table('columnar_tbl', NULL, colocate_with:='none');
 create_distributed_table
---------------------------------------------------------------------

(1 row)

SELECT * FROM columnar.options WHERE relation = 'columnar_tbl'::regclass;
   relation   | chunk_group_row_limit | stripe_row_limit | compression | compression_level
---------------------------------------------------------------------
 columnar_tbl |                 10000 |           150000 | zstd        |                 3
(1 row)

SELECT alter_columnar_table_set('columnar_tbl', compression_level => 2);
 alter_columnar_table_set
---------------------------------------------------------------------

(1 row)

SELECT * FROM columnar.options WHERE relation = 'columnar_tbl'::regclass;
   relation   | chunk_group_row_limit | stripe_row_limit | compression | compression_level
---------------------------------------------------------------------
 columnar_tbl |                 10000 |           150000 | zstd        |                 2
(1 row)

SELECT alter_columnar_table_reset('columnar_tbl', compression_level => true);
 alter_columnar_table_reset
---------------------------------------------------------------------

(1 row)

SELECT * FROM columnar.options WHERE relation = 'columnar_tbl'::regclass;
   relation   | chunk_group_row_limit | stripe_row_limit | compression | compression_level
---------------------------------------------------------------------
 columnar_tbl |                 10000 |           150000 | zstd        |                 3
(1 row)

SELECT columnar_internal.upgrade_columnar_storage(c.oid)
FROM pg_class c, pg_am a
WHERE c.relam = a.oid AND amname = 'columnar' AND relname = 'columnar_tbl';
 upgrade_columnar_storage
---------------------------------------------------------------------

(1 row)

SELECT columnar_internal.downgrade_columnar_storage(c.oid)
FROM pg_class c, pg_am a
WHERE c.relam = a.oid AND amname = 'columnar' AND relname = 'columnar_tbl';
 downgrade_columnar_storage
---------------------------------------------------------------------

(1 row)

CREATE OR REPLACE FUNCTION columnar_storage_info(
    rel regclass,
    version_major OUT int4,
    version_minor OUT int4,
    storage_id OUT int8,
    reserved_stripe_id OUT int8,
    reserved_row_number OUT int8,
    reserved_offset OUT int8)
  STRICT
  LANGUAGE c AS 'citus', $$columnar_storage_info$$;
SELECT version_major, version_minor, reserved_stripe_id, reserved_row_number, reserved_offset FROM columnar_storage_info('columnar_tbl');
 version_major | version_minor | reserved_stripe_id | reserved_row_number | reserved_offset
---------------------------------------------------------------------
             2 |             0 |                  1 |                   1 |           16336
(1 row)

SELECT columnar.get_storage_id(oid) = storage_id FROM pg_class, columnar_storage_info('columnar_tbl') WHERE relname = 'columnar_tbl';
 ?column?
---------------------------------------------------------------------
 t
(1 row)

-- test time series functions
CREATE TABLE part_tbl (a DATE) PARTITION BY RANGE (a);
CREATE TABLE part_tbl_1 PARTITION OF part_tbl FOR VALUES FROM ('2000-01-01') TO ('2010-01-01');
CREATE TABLE part_tbl_2 PARTITION OF part_tbl FOR VALUES FROM ('2020-01-01') TO ('2030-01-01');
SELECT create_distributed_table('part_tbl', NULL, colocate_with:='none');
 create_distributed_table
---------------------------------------------------------------------

(1 row)

SELECT * FROM time_partitions WHERE parent_table::text = 'part_tbl';
 parent_table | partition_column | partition  | from_value |  to_value  | access_method
---------------------------------------------------------------------
 part_tbl     | a                | part_tbl_1 | 01-01-2000 | 01-01-2010 | heap
 part_tbl     | a                | part_tbl_2 | 01-01-2020 | 01-01-2030 | heap
(2 rows)

SELECT time_partition_range('part_tbl_2');
  time_partition_range
---------------------------------------------------------------------
 (01-01-2020,01-01-2030)
(1 row)

SELECT get_missing_time_partition_ranges('part_tbl', INTERVAL '10 years', '2050-01-01', '2000-01-01');
   get_missing_time_partition_ranges
---------------------------------------------------------------------
 (part_tbl_p2010,01-01-2010,01-01-2020)
 (part_tbl_p2030,01-01-2030,01-01-2040)
 (part_tbl_p2040,01-01-2040,01-01-2050)
(3 rows)

SELECT create_time_partitions('part_tbl', INTERVAL '10 years', '2050-01-01', '2000-01-01');
 create_time_partitions
---------------------------------------------------------------------
 t
(1 row)

CALL drop_old_time_partitions('part_tbl', '2030-01-01');
NOTICE:  dropping part_tbl_1 with start time 01-01-2000 and end time 01-01-2010
CONTEXT:  PL/pgSQL function drop_old_time_partitions(regclass,timestamp with time zone) line XX at RAISE
NOTICE:  dropping part_tbl_p2010 with start time 01-01-2010 and end time 01-01-2020
CONTEXT:  PL/pgSQL function drop_old_time_partitions(regclass,timestamp with time zone) line XX at RAISE
NOTICE:  dropping part_tbl_2 with start time 01-01-2020 and end time 01-01-2030
CONTEXT:  PL/pgSQL function drop_old_time_partitions(regclass,timestamp with time zone) line XX at RAISE
SELECT * FROM time_partitions WHERE parent_table::text = 'part_tbl';
 parent_table | partition_column |   partition    | from_value |  to_value  | access_method
---------------------------------------------------------------------
 part_tbl     | a                | part_tbl_p2030 | 01-01-2030 | 01-01-2040 | heap
 part_tbl     | a                | part_tbl_p2040 | 01-01-2040 | 01-01-2050 | heap
(2 rows)

-- test locking shards
CREATE TABLE lock_tbl_1 (a INT);
SELECT create_distributed_table('lock_tbl_1', NULL, colocate_with:='none');
 create_distributed_table
---------------------------------------------------------------------

(1 row)

CREATE TABLE lock_tbl_2 (a INT);
SELECT create_distributed_table('lock_tbl_2', NULL, colocate_with:='none');
 create_distributed_table
---------------------------------------------------------------------

(1 row)

BEGIN;
SELECT lock_shard_metadata(3, array_agg(distinct(shardid)))
FROM citus_shards WHERE table_name::text = 'lock_tbl_1';
 lock_shard_metadata
---------------------------------------------------------------------

(1 row)

SELECT lock_shard_metadata(5, array_agg(distinct(shardid)))
FROM citus_shards WHERE table_name::text LIKE 'lock\_tbl\__';
 lock_shard_metadata
---------------------------------------------------------------------

(1 row)

SELECT table_name, classid, mode, granted
FROM pg_locks, public.citus_tables
WHERE
      locktype = 'advisory' AND
      table_name::text LIKE 'lock\_tbl\__' AND
      objid = colocation_id
      ORDER BY 1, 3;
 table_name | classid |       mode       | granted
---------------------------------------------------------------------
 lock_tbl_1 |       0 | RowExclusiveLock | t
 lock_tbl_1 |       0 | ShareLock        | t
 lock_tbl_2 |       0 | ShareLock        | t
(3 rows)

END;
BEGIN;
SELECT lock_shard_resources(3, array_agg(distinct(shardid)))
FROM citus_shards WHERE table_name::text = 'lock_tbl_1';
 lock_shard_resources
---------------------------------------------------------------------

(1 row)

SELECT lock_shard_resources(5, array_agg(distinct(shardid)))
FROM citus_shards WHERE table_name::text LIKE 'lock\_tbl\__';
 lock_shard_resources
---------------------------------------------------------------------

(1 row)

SELECT locktype, table_name, mode, granted
FROM pg_locks, citus_shards, pg_dist_node
WHERE
      objid = shardid AND
      table_name::text LIKE 'lock\_tbl\__' AND
      citus_shards.nodeport = pg_dist_node.nodeport AND
      noderole = 'primary'
      ORDER BY 2, 3;
 locktype | table_name |       mode       | granted
---------------------------------------------------------------------
 advisory | lock_tbl_1 | RowExclusiveLock | t
 advisory | lock_tbl_1 | ShareLock        | t
 advisory | lock_tbl_2 | ShareLock        | t
(3 rows)

END;
SET client_min_messages TO WARNING;
DROP SCHEMA null_dist_key_udfs CASCADE;
